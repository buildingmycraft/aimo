{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "16f4137c",
   "metadata": {},
   "source": [
    "# Starter Notebook for vLLM inference\n",
    "\n",
    "This notebook provides an outline for a very basic inference setup using vLLM. It does not have any performance optimizations for faster or memory efficient inference, required prompt engineering techniques (e.g. chain of thought) or model tuning. The goal is to have a starting base for inference that successfully runs from start to finish.\n",
    "\n",
    "Our target problem will be solving math problems from the Math QSA dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a9fbe5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import re\n",
    "from IPython.display import display, Markdown\n",
    "import pandas as pd\n",
    "from vllm import LLM, SamplingParams"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0081876f",
   "metadata": {},
   "source": [
    "We'll set some initial parameters for our inference pipeline:\n",
    "- `n_shots`: we'll use few-shot prompting using `n_shots`\n",
    "- `n_samples`: given our goal is to merely see the pipeline run, we'll only run inference on an `n_samples` subset of our available data\n",
    "- `model_id`: the huggingface model we're using for inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "292ccc25",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_shots = 2\n",
    "n_samples = 100\n",
    "model_id = \"deepseek-ai/deepseek-math-7b-rl\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a874c1b",
   "metadata": {},
   "source": [
    "## Load Math Dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce8cd182",
   "metadata": {},
   "source": [
    "We'll be using [awsaf49/math-qsa-dataset](https://www.kaggle.com/datasets/awsaf49/math-qsa-dataset) as the dataset to run inference on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c308dc1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load math-qsa-dataset\n",
    "# use glob for future scenarios where multiple files are loaded using pattern matching\n",
    "# in this scenario, it's unnecessary as we're only loading one file\n",
    "paths = glob.glob(\"../external-data/math-qsa-dataset/train.csv\")  \n",
    "math_qsa_df = pd.concat([pd.read_csv(path) for path in paths]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "02a79779",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>problem</th>\n",
       "      <th>level</th>\n",
       "      <th>type</th>\n",
       "      <th>solution</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The United States Postal Service charges an ex...</td>\n",
       "      <td>Level 3</td>\n",
       "      <td>Prealgebra</td>\n",
       "      <td>We calculate the desired ratio for each envelo...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>How many integers between 1000 and 2000 have a...</td>\n",
       "      <td>Level 4</td>\n",
       "      <td>Prealgebra</td>\n",
       "      <td>A number with 15, 20 and 25 as factors must be...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Given that $n$ is an integer and $0 &lt; 4n &lt;30$,...</td>\n",
       "      <td>Level 2</td>\n",
       "      <td>Prealgebra</td>\n",
       "      <td>Dividing by $4$, we have $0&lt;n&lt;7\\frac{1}{2}$. T...</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>How many integers between $100$ and $150$ have...</td>\n",
       "      <td>Level 4</td>\n",
       "      <td>Prealgebra</td>\n",
       "      <td>We will break up the problem into cases based ...</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Regular pentagon $ABCDE$ and regular hexagon $...</td>\n",
       "      <td>Level 4</td>\n",
       "      <td>Prealgebra</td>\n",
       "      <td>We know that the sum of the degree measures of...</td>\n",
       "      <td>132</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             problem    level        type  \\\n",
       "0  The United States Postal Service charges an ex...  Level 3  Prealgebra   \n",
       "1  How many integers between 1000 and 2000 have a...  Level 4  Prealgebra   \n",
       "2  Given that $n$ is an integer and $0 < 4n <30$,...  Level 2  Prealgebra   \n",
       "3  How many integers between $100$ and $150$ have...  Level 4  Prealgebra   \n",
       "4  Regular pentagon $ABCDE$ and regular hexagon $...  Level 4  Prealgebra   \n",
       "\n",
       "                                            solution answer  \n",
       "0  We calculate the desired ratio for each envelo...      3  \n",
       "1  A number with 15, 20 and 25 as factors must be...      3  \n",
       "2  Dividing by $4$, we have $0<n<7\\frac{1}{2}$. T...     28  \n",
       "3  We will break up the problem into cases based ...     18  \n",
       "4  We know that the sum of the degree measures of...    132  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "math_qsa_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e35a6fd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7498 entries, 0 to 7497\n",
      "Data columns (total 5 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   problem   7498 non-null   object\n",
      " 1   level     7498 non-null   object\n",
      " 2   type      7498 non-null   object\n",
      " 3   solution  7498 non-null   object\n",
      " 4   answer    7496 non-null   object\n",
      "dtypes: object(5)\n",
      "memory usage: 293.0+ KB\n"
     ]
    }
   ],
   "source": [
    "math_qsa_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25a35d5b",
   "metadata": {},
   "source": [
    "## Data Cleaning\n",
    "\n",
    "We need to do some very basic data cleaning on the math-qsa dataset to remove rows that are missing a parsed answer and trim any excess whitespace from the parsed answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6b06e606",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_114616/4173954450.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[\"answer\"] = df[\"answer\"].str.strip()  # trim whitespace\n"
     ]
    }
   ],
   "source": [
    "def process_data(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.query(\"answer == answer\")  # remove nans\n",
    "    df[\"answer\"] = df[\"answer\"].str.strip()  # trim whitespace\n",
    "    return df\n",
    "\n",
    "math_qsa_df = process_data(math_qsa_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f08b8c6",
   "metadata": {},
   "source": [
    "We also remove _n_ example problems from the dataset to use as examples for few-shot prompting (in-context learning). Note that we haven't actually run any experiments to validate the use of few-shot prompting nor optimize the size of _n_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "401ed28c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>problem</th>\n",
       "      <th>level</th>\n",
       "      <th>type</th>\n",
       "      <th>solution</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The United States Postal Service charges an ex...</td>\n",
       "      <td>Level 3</td>\n",
       "      <td>Prealgebra</td>\n",
       "      <td>We calculate the desired ratio for each envelo...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>How many integers between 1000 and 2000 have a...</td>\n",
       "      <td>Level 4</td>\n",
       "      <td>Prealgebra</td>\n",
       "      <td>A number with 15, 20 and 25 as factors must be...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             problem    level        type  \\\n",
       "0  The United States Postal Service charges an ex...  Level 3  Prealgebra   \n",
       "1  How many integers between 1000 and 2000 have a...  Level 4  Prealgebra   \n",
       "\n",
       "                                            solution answer  \n",
       "0  We calculate the desired ratio for each envelo...      3  \n",
       "1  A number with 15, 20 and 25 as factors must be...      3  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_shot_examples = math_qsa_df[:n_shots].copy(); n_shot_examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1899f8b0",
   "metadata": {},
   "source": [
    "We'll take a random subset of `n_samples` to test our model inference setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "005881aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_df = math_qsa_df[n_shots:].sample(n_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9da08034",
   "metadata": {},
   "source": [
    "## Prompt Template\n",
    "\n",
    "Now let's setup our prompting template."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "149bff34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt templating setup\n",
    "# starting template taken from https://www.kaggle.com/code/awsaf49/aimo-kerasnlp-starter/notebook\n",
    "# with some minor adjustments e.g. adding few-shot prompting\n",
    "\n",
    "# TODO: clean up implementation by using a templating engine like Jinja\n",
    "# using a proper templating engine allow the templating logic to be embedded in the template string\n",
    "# current implementation splits the implementation logic between the template and template generation functions\n",
    "# meaning they need to be updated in tandem with each other\n",
    "\n",
    "ROLE_TEMPLATE = \"\"\"Role:\n",
    "You are an advanced AI system with exceptional mathematical reasoning and problem-solving capabilities, specifically designed to solve tricky math problems (whose answer is a non-negative integer) written in LaTeX format from the AI Mathematical Olympiad (AIMO) competition. Your task is to accurately analyze and solve intricate mathematical problems, demonstrating a deep understanding of mathematical concepts and a strong ability to apply logical reasoning strategies.\n",
    "\n",
    "Instruction:\n",
    "1. Carefully read and comprehend the problem statement provided in the \"Problem\" section.\n",
    "2. In the \"Solution\" section, provide a solution of the problem with detailed explanation of your logical reasoning process.\n",
    "3. Mark your final answer by writing it within the \\\\boxed{} LaTeX operator.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "QUESTION_SOLUTION_TEMPLATE = \"\"\"Problem:\n",
    "{problem}\n",
    "\n",
    "Solution:\n",
    "{solution}\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "N_SHOT_EXAMPLES = \"\".join([\n",
    "    QUESTION_SOLUTION_TEMPLATE.format(**example.to_dict())\n",
    "    for _, example in n_shot_examples.iterrows()\n",
    "])\n",
    "\n",
    "QUESTION_TEMPLATE = \"\"\"Problem:\n",
    "{problem}\n",
    "\n",
    "Solution:\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def generate_ground_truth(instance: pd.Series) -> str:\n",
    "    instance_str = QUESTION_SOLUTION_TEMPLATE.format(**instance.to_dict())\n",
    "    return ROLE_TEMPLATE + N_SHOT_EXAMPLES + instance_str\n",
    "\n",
    "def generate_prompt(instance: pd.Series) -> str:\n",
    "    instance_str = QUESTION_TEMPLATE.format(**instance.to_dict())\n",
    "    return ROLE_TEMPLATE + N_SHOT_EXAMPLES + instance_str"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "250008b2",
   "metadata": {},
   "source": [
    "### Example of rendering prompt and ground truth output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d9fbce39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Role:\n",
       "You are an advanced AI system with exceptional mathematical reasoning and problem-solving capabilities, specifically designed to solve tricky math problems (whose answer is a non-negative integer) written in LaTeX format from the AI Mathematical Olympiad (AIMO) competition. Your task is to accurately analyze and solve intricate mathematical problems, demonstrating a deep understanding of mathematical concepts and a strong ability to apply logical reasoning strategies.\n",
       "\n",
       "Instruction:\n",
       "1. Carefully read and comprehend the problem statement provided in the \"Problem\" section.\n",
       "2. In the \"Solution\" section, provide a solution of the problem with detailed explanation of your logical reasoning process.\n",
       "3. Mark your final answer by writing it within the \\boxed{} LaTeX operator.\n",
       "\n",
       "Problem:\n",
       "The United States Postal Service charges an extra $\\$0.11$ in postage if the length of an envelope, in inches, divided by its height, in inches, is less than $1.3$ or greater than $2.5.$ For how many of these four envelopes must the extra $\\$0.11$ in postage be paid? \\begin{tabular}[t]{ccc}\n",
       "Envelope & Length in inches & Height in inches\\\\\\hline\n",
       "A &6 &4\\\\\n",
       "B &9 &3\\\\\n",
       "C &6 &6\\\\\n",
       "D &11 &4\n",
       "\\end{tabular}\n",
       "\n",
       "Solution:\n",
       "We calculate the desired ratio for each envelope: \\begin{align*}\n",
       "\\text{A} &= \\frac{6}{4} = 1.5 \\\\\n",
       "\\text{B} &= \\frac{9}{3} = 3 \\\\\n",
       "\\text{C} &= \\frac{6}{6} = 1 \\\\\n",
       "\\text{D} &= \\frac{11}{4} = 2.75\n",
       "\\end{align*} $\\text B,$ $\\text C,$ and $\\text D$ are out of range, so the answer is $\\boxed{3}.$\n",
       "\n",
       "Problem:\n",
       "How many integers between 1000 and 2000 have all three of the numbers 15, 20 and 25 as factors?\n",
       "\n",
       "Solution:\n",
       "A number with 15, 20 and 25 as factors must be divisible by their least common multiple (LCM).  Because $15 = 3\n",
       "\\times 5$, $20 = 2^2 \\times 5$, and $25 = 5^2$, the LCM of 15, 20 and 25 is $2^2 \\times 3 \\times 5^2 = 300$. There are $\\boxed{3}$ multiples of 300 between 1000 and 2000: 1200, 1500 and 1800.\n",
       "\n",
       "Problem:\n",
       "For what digit $d$ is the five-digit number $2345d$ a multiple of 9?\n",
       "\n",
       "Solution:\n",
       "In order for a number to be a multiple of 9, the sum of its digits must be divisible by 9. Since $2+3+4+5=14$, the only single digit that will make the sum a multiple of 9 is $4$. The sum of the digits would be $18$, which is $9\\cdot 2$, so $d=\\boxed{4}$.\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "example_ground_truth = generate_ground_truth(instance=inference_df.iloc[0])\n",
    "display(Markdown(example_ground_truth))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "742ee3e3",
   "metadata": {},
   "source": [
    "### Example of prompt created for inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "63fb12d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Role:\n",
       "You are an advanced AI system with exceptional mathematical reasoning and problem-solving capabilities, specifically designed to solve tricky math problems (whose answer is a non-negative integer) written in LaTeX format from the AI Mathematical Olympiad (AIMO) competition. Your task is to accurately analyze and solve intricate mathematical problems, demonstrating a deep understanding of mathematical concepts and a strong ability to apply logical reasoning strategies.\n",
       "\n",
       "Instruction:\n",
       "1. Carefully read and comprehend the problem statement provided in the \"Problem\" section.\n",
       "2. In the \"Solution\" section, provide a solution of the problem with detailed explanation of your logical reasoning process.\n",
       "3. Mark your final answer by writing it within the \\boxed{} LaTeX operator.\n",
       "\n",
       "Problem:\n",
       "The United States Postal Service charges an extra $\\$0.11$ in postage if the length of an envelope, in inches, divided by its height, in inches, is less than $1.3$ or greater than $2.5.$ For how many of these four envelopes must the extra $\\$0.11$ in postage be paid? \\begin{tabular}[t]{ccc}\n",
       "Envelope & Length in inches & Height in inches\\\\\\hline\n",
       "A &6 &4\\\\\n",
       "B &9 &3\\\\\n",
       "C &6 &6\\\\\n",
       "D &11 &4\n",
       "\\end{tabular}\n",
       "\n",
       "Solution:\n",
       "We calculate the desired ratio for each envelope: \\begin{align*}\n",
       "\\text{A} &= \\frac{6}{4} = 1.5 \\\\\n",
       "\\text{B} &= \\frac{9}{3} = 3 \\\\\n",
       "\\text{C} &= \\frac{6}{6} = 1 \\\\\n",
       "\\text{D} &= \\frac{11}{4} = 2.75\n",
       "\\end{align*} $\\text B,$ $\\text C,$ and $\\text D$ are out of range, so the answer is $\\boxed{3}.$\n",
       "\n",
       "Problem:\n",
       "How many integers between 1000 and 2000 have all three of the numbers 15, 20 and 25 as factors?\n",
       "\n",
       "Solution:\n",
       "A number with 15, 20 and 25 as factors must be divisible by their least common multiple (LCM).  Because $15 = 3\n",
       "\\times 5$, $20 = 2^2 \\times 5$, and $25 = 5^2$, the LCM of 15, 20 and 25 is $2^2 \\times 3 \\times 5^2 = 300$. There are $\\boxed{3}$ multiples of 300 between 1000 and 2000: 1200, 1500 and 1800.\n",
       "\n",
       "Problem:\n",
       "For what digit $d$ is the five-digit number $2345d$ a multiple of 9?\n",
       "\n",
       "Solution:\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "example_prompt = generate_prompt(instance=inference_df.iloc[0])\n",
    "display(Markdown(example_prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acb09a58",
   "metadata": {},
   "source": [
    "## Run inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47461cbd",
   "metadata": {},
   "source": [
    "We'll load the pre-trained model, run inference on our prompts and store the generated results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7169986d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate batch of prompts\n",
    "inference_df[\"prompt\"] = inference_df.apply(generate_prompt, axis=1)\n",
    "prompts = inference_df[\"prompt\"].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "afc0a52e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-12 10:24:14 llm_engine.py:237] Initializing an LLM engine (v0.6.3.post1) with config: model='deepseek-ai/deepseek-math-7b-rl', speculative_config=None, tokenizer='deepseek-ai/deepseek-math-7b-rl', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=deepseek-ai/deepseek-math-7b-rl, num_scheduler_steps=1, chunked_prefill_enabled=False multi_step_stream_outputs=True, enable_prefix_caching=False, use_async_output_proc=True, use_cached_outputs=False, mm_processor_kwargs=None)\n",
      "INFO 11-12 10:24:15 model_runner.py:1056] Starting to load model deepseek-ai/deepseek-math-7b-rl...\n",
      "INFO 11-12 10:24:15 weight_utils.py:243] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5be36afc03840a8aef5866b0fd94712",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-12 10:24:17 model_runner.py:1067] Loading model weights took 12.8725 GB\n",
      "INFO 11-12 10:24:18 gpu_executor.py:122] # GPU blocks: 980, # CPU blocks: 546\n",
      "INFO 11-12 10:24:18 gpu_executor.py:126] Maximum concurrency for 4096 tokens per request: 3.83x\n",
      "INFO 11-12 10:24:20 model_runner.py:1395] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 11-12 10:24:20 model_runner.py:1399] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 11-12 10:24:30 model_runner.py:1523] Graph capturing finished in 10 secs.\n"
     ]
    }
   ],
   "source": [
    "# load pre-trained model\n",
    "llm = LLM(model=model_id, trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3f317bc4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processed prompts:   0%|        | 0/100 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-12 10:24:33 scheduler.py:1483] Sequence group 22 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|â–ˆ| 100/100 [01:11<00:00,  1.40it/s, est. speed input: 971.56 toks/s, output: 385.89 \n"
     ]
    }
   ],
   "source": [
    "# run inference\n",
    "sampling_params = SamplingParams(temperature=0.8, top_p=0.95, max_tokens=500)\n",
    "outputs = llm.generate(prompts, sampling_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eea801d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# store generated solutions\n",
    "generated_text = [output.outputs[0].text for output in outputs]\n",
    "inference_df[\"generated_solution\"] = generated_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "757465b6",
   "metadata": {},
   "source": [
    "## Extract answers from generated solution text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "727c9719",
   "metadata": {},
   "source": [
    "Our ground truth and few shot examples provide the final answer inside of the \\\\boxed{<enclosed>} LaTeX command. We'll expect our text generations to use this format and therefore need to parse the solution to extract the final answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "32060922",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract answers from generated solutions\n",
    "def extract_answer(text: str) -> str:\n",
    "    \"\"\"Given an input text, matches \\\\boxed{<enclosed>} and returns <enclosed>\"\"\"\n",
    "    start = text.find(\"\\\\boxed{\")\n",
    "    if start < 0:\n",
    "        return None\n",
    "    \n",
    "    answer = \"\"\n",
    "    open_brace_count = 1\n",
    "    for c in text[start+7:]:\n",
    "        if c == \"{\":\n",
    "            open_brace_count += 1\n",
    "        elif c == \"}\":\n",
    "            open_brace_count -= 1\n",
    "            if open_brace_count == 0:\n",
    "                break\n",
    "        answer += c\n",
    "    return answer.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2952c3ff",
   "metadata": {},
   "source": [
    "Let's test our answer extraction logic against the ground truth solutions to confirm that it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5ee47bab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 100 matches and 0 mismatches\n",
      "Mismatches:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>problem</th>\n",
       "      <th>level</th>\n",
       "      <th>type</th>\n",
       "      <th>solution</th>\n",
       "      <th>answer</th>\n",
       "      <th>prompt</th>\n",
       "      <th>generated_solution</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [problem, level, type, solution, answer, prompt, generated_solution]\n",
       "Index: []"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test extraction logic on training dataset\n",
    "extracted_answers = inference_df[\"solution\"].apply(extract_answer)\n",
    "matched_answers = extracted_answers == inference_df[\"answer\"]\n",
    "\n",
    "print(f\"Found {sum(matched_answers)} matches and {sum(~matched_answers)} mismatches\")\n",
    "print(\"Mismatches:\")\n",
    "inference_df[~matched_answers]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fbc6efd",
   "metadata": {},
   "source": [
    "Now let's parse our generated solutions and calculate our accuracy against the ground truth answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bf39b4e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_df[\"generated_answer\"] = inference_df[\"generated_solution\"].apply(extract_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6e9da6fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy=0.25\n"
     ]
    }
   ],
   "source": [
    "# calculate accuracy - # answers generated, # correct answers\n",
    "accuracy = inference_df.pipe(lambda x: (x[\"answer\"] == x[\"generated_answer\"]).sum() / x[\"answer\"].count())\n",
    "print(f\"{accuracy=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "723528fa",
   "metadata": {},
   "source": [
    "For what proportion of our prompts were we actually able to extract an answer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3b811e7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "missing_answer=0.62\n"
     ]
    }
   ],
   "source": [
    "missing_answer = inference_df.pipe(lambda x: 1 - x[\"generated_answer\"].count() / x[\"answer\"].count())\n",
    "print(f\"{missing_answer=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be635bc2",
   "metadata": {},
   "source": [
    "Most of our generated solutions either do not provide a final answer or fail to use the ``\\boxed`` command. This seems like low hanging fruit for future improvement."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:aimo]",
   "language": "python",
   "name": "conda-env-aimo-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
